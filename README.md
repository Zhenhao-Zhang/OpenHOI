# [NeurIPS 2025 Oral] OpenHOI: Open-World Hand-Object Interaction Synthesis with Multimodal Large Language Model

This is the offical code repo for **NeurIPS 2025 Oral** paper **OpenHOI: Open-World Hand-Object Interaction Synthesis with Multimodal Large Language Model**

[[paper]](https://arxiv.org/abs/2505.18947) [[project page]](http://openhoi.github.io/)

<div align="center">
    <img src="pipeline.png" height=500>
</div>



**We will release our code as soon as possible**

Any Question, feel free to contact zhangzhh2024@shanghaitech.edu.cn

# Acknowledgement
Thanks for the excellent work [ShapeLLM](https://github.com/qizekun/ShapeLLM/),[Text2HOI](https://github.com/JunukCha/Text2HOI),[DSG](https://github.com/LingxiaoYang2023/DSG2024),[SeqAfford](https://github.com/hq-King/SeqAfford),[GazeHOI](https://github.com/takiee/GazeHOI-toolkit)


# Citation

If you find our work useful in your research, please consider citing

```
@inproceedings{
zhang2025openhoi,
title={Open{HOI}: Open-World Hand-Object Interaction Synthesis with Multimodal Large Language Model},
author={Zhenhao Zhang and Ye Shi and Lingxiao Yang and Suting Ni and Qi Ye and Jingya Wang},
booktitle={The Thirty-ninth Annual Conference on Neural Information Processing Systems},
year={2025},
}
```