Metadata-Version: 2.4
Name: llava
Version: 1.1.3
Summary: Towards GPT-4 like large language and visual assistant.
Project-URL: Homepage, https://qizekun.github.io/shapellm
Project-URL: Bug Tracker, https://github.com/qizekun/ShapeLLM/issues
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: Apache Software License
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: torch==2.0.1
Requires-Dist: torchvision==0.15.2
Requires-Dist: transformers==4.31.0
Requires-Dist: tokenizers<0.14,>=0.12.1
Requires-Dist: sentencepiece==0.1.99
Requires-Dist: shortuuid
Requires-Dist: accelerate==0.21.0
Requires-Dist: peft==0.4.0
Requires-Dist: bitsandbytes==0.41.0
Requires-Dist: pydantic<2,>=1
Requires-Dist: markdown2[all]
Requires-Dist: numpy==1.26.4
Requires-Dist: scikit-learn==1.2.2
Requires-Dist: gradio==3.35.2
Requires-Dist: gradio_client==0.2.9
Requires-Dist: requests
Requires-Dist: httpx==0.24.0
Requires-Dist: uvicorn
Requires-Dist: fastapi
Requires-Dist: einops==0.6.1
Requires-Dist: einops-exts==0.0.4
Requires-Dist: timm==0.9.12
Requires-Dist: argparse
Requires-Dist: easydict
Requires-Dist: h5py
Requires-Dist: matplotlib
Requires-Dist: tqdm
Requires-Dist: opencv-python==4.10.0.84
Requires-Dist: pyyaml
Requires-Dist: scipy
Requires-Dist: tensorboardX
Requires-Dist: termcolor
Requires-Dist: pandas
Requires-Dist: ftfy
Requires-Dist: regex
Requires-Dist: plyfile
Requires-Dist: ipdb
Requires-Dist: jsonlines
Requires-Dist: openai
Requires-Dist: nltk
Requires-Dist: rouge
Requires-Dist: py-rouge
Provides-Extra: train
Requires-Dist: deepspeed==0.9.5; extra == "train"
Requires-Dist: ninja; extra == "train"
Requires-Dist: wandb; extra == "train"
Requires-Dist: torch-scatter==2.0.9; extra == "train"

<br>
<p align="center">
<h1 align="center"><strong>SeqAfford: Sequential 3D Affordance Reasoning via Multimodal Large Language Model
</strong></h1>
  <p align="center">
      <strong><span style="color: red;">CVPR 2025</span></strong>
    <br>
   Chunlin Yu*</a>&emsp;
    <a href='https://hq-King.github.io' target='_blank'>Hanqing Wang*</a>&emsp;
   Ye Shi</a>&emsp;
   Haoyang Luo</a>&emsp;
    Sibei Yang</a>&emsp;
   Jingyi Yu</a>&emsp;
   Jingya Wang</a>&emsp;
    <br>
    ShanghaiTech University    
    <br>
    *Indicates Equal Contribution
    <br>
  </p>
</p>

  

<p align="center">
  <a href="https://seq-afford.github.io"><b>ğŸ“– Project Page</b></a> |
  <a href="https://arxiv.org/pdf/2412.01550"><b>ğŸ“„ Paper Link</b></a> |
</p>

</div>

> We introduce SeqAfford, a Multi-Modal Language Model (MLLM) capable of serialized affordance inference implied in human instructions: 1) Single Affordance Reasoning; 2) Sequential Affordance Reasoning; 3) Sequential Affordance Reasoning with Multiple Objects

<div align="center">
    <img src="fig1.png" height=500>
</div>

## ğŸ“£ News
- [2/27/2025] ğŸ‰ğŸ‰ğŸ‰SeqAfford has been accepted by CVPR 2025!!!ğŸ‰ğŸ‰ğŸ‰
- [12/2/2024] SeqAfford has been released on Arxiv now!!!

## ğŸ˜² Results
Please refer to our [homepage](https://seq-afford.github.io) for more thrilling results!


## ğŸ› ï¸ Setup
- 1. Create a new `conda` environment and activate it by following command
  ```bash
  conda env create -f environment.yaml
  ```
- 2. Down [ShapeLLM](https://github.com/qizekun/ShapeLLM/blob/main/docs/MODEL_ZOO.md) model weight into your directory, and Modify the model path in the `scripts/finetune_lora.sh`ï¼Œ including both `--vision_tower_path` and `--pretrain_mm_mlp_adapter`
 
 - 3. Down [Uni3D](https://github.com/baaivision/Uni3D) model weight into your directory, and Modify the model path in the `./llava/model/language_model/affordancellm.py`
 
- 4. you can train your own model by running the following code
 
```bash
  sh ./scripts/finetune_lora.sh
```
# ğŸ“š Data
visit the link to download the [Dataset](https://pan.baidu.com/s/1_koVmNMdv5BByli97eDHGA?pwd=2025)

## ğŸš© Plan
- [x] Paper Released.
- [âˆš ] Source Code and Pretrained Weights.
- [âˆš ] Dataset.
<!-- --- -->

## Acknowledgement
Thanks for the wonderful works: [ShapeLLM](https://github.com/qizekun/ShapeLLM/blob/main/docs/MODEL_ZOO.md), [LISA](https://github.com/dvlab-research/LISA), This work is built upon them.

## ğŸ« License

For academic use, this project is licensed under [the 2-clause BSD License](https://opensource.org/license/bsd-2-clause). 

## ğŸ–Šï¸ Citation
```
@article{yu2024seqafford,
        title={SeqAfford: Sequential 3D Affordance Reasoning via Multimodal Large Language Model},
        author={Yu, Chunlin and Wang, Hanqing and Shi, Ye and Luo, Haoyang and Yang, Sibei and Yu, Jingyi and Wang, Jingya},
        journal={arXiv preprint arXiv:2412.01550},
        year={2024}
      }

```
